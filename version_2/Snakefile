# snakemake/5.17.0
# pipeline to test the presence of some proteins (aka seeds) in genome (given in taxid)
# usage: snakemake --cluster-config cluster.json --cluster "sbatch -c {cluster.c} --qos={cluster.qos} --time={cluster.time} --account={cluster.account} --mail-type={cluster.mail-type} --mail-user={cluster.mail-user} --mem_per_cpu={cluster.mem_per_cpu}"  -j 5 -d "/blue/lagard/ghutinet/modifications/trial_second" -C project_name='deazaguanine' e_val=0.01 id=0.1 cov=0.7 taxid='base_set_taxid.txt'
# -j : number of max core to use
# -d : directory of the files
# -C : configuration of values bellow:

# ###############################################################################
# This file is part of Presence-Abscence.                                       #
#                                                                               #
# Authors: Geoffrey Hutinet                                                     #
# Copyright Â© 2021 University of Forida (Gainesville, Florida).                 #
# See the COPYRIGHT file for details.                                           #
#                                                                               #
# Presence-Abscence is a software providing tools for ???                       #
#                                                                               #
# Presence-Abscence is free software: you can redistribute it and/or modify     #
# it under the terms of the Affero GNU General Public License as published by   #
# the Free Software Foundation, either version 3 of the License,                #
# or (at your option) any later version.                                        #
#                                                                               #
# Presence-Abscence is distributed in the hope that it will be useful, but      #
# WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY    #
# or FITNESSFOR A PARTICULAR PURPOSE. See the Affero GNU General Public License #
# for more details.                                                             #
#                                                                               #
# You should have received a copy of the Affero GNU General Public License      #
# along with Presence-Abscence (COPYING file).                                  #
# If not, see <https://www.gnu.org/licenses/>.                                  #
# ###############################################################################

##########################################################################
##########################################################################
##
##                            Singularity
##
##########################################################################
##########################################################################


singularity: "https://github.com/vdclab/simg-PA_tools/releases/download/0.0.2/vdclab-simg-PA_tools.latest.sif"

##########################################################################
##########################################################################
##
##                                Library
##
##########################################################################
##########################################################################

import os, sys
import pandas as pd
from textwrap import dedent

##########################################################################
##########################################################################
##
##                                Options
##
##########################################################################
##########################################################################

# Taxid necessary to work
if 'taxid' not in config:
    sys.exit(dedent("""usage: snakemake [option] -C seed=<SEED_FILE> taxid=<TAXID_FILE>',
    error: one of this arguments are required: You need to provide a taxid file"""))
else :
    taxid = config['taxid']

# Seed file input
if 'seed' not in config :
    sys.exit(dedent("""usage: snakemake [option] -C seed=<SEED_FILE> taxid=<TAXID_FILE>',
    error: one of this arguments are required: You need to provide a seed file"""))
else :
    seed_file = config['seed'] 

# Name your project, take the name of seed by default
project_name = config['project_name'] if 'project_name' in config else os.path.splitext(seed_file)[0]



# Blast e-value thershold, 0.000001 by default but can be changed in -C
e_val = config['e_val'] if 'e_val' in config else 0.000001

# Software versions
blast_version = config['blast'] if 'blast' in config else '2.10.1'
silix_version = config['silix'] if 'silix' in config else '1.2.11'

# Option for ncbi_genome_download
# Values for section : {refseq,genbank}
section = config['section'] if 'section' in config else 'genbank'

# Values for assembly_levels : ['all', 'complete', 'chromosome', 'scaffold', 'contig']
assembly_levels = config['assembly_levels'] if 'assembly_levels' in config else 'all'

# Values for refseq_categories : {'reference', 'all'}
refseq_categories = config['refseq_categories'] if 'refseq_categories' in config else 'all'

# Values for groups : ['all', 'archaea', 'bacteria', 'fungi', 'invertebrate', 'metagenomes', 'plant', 'protozoa', 'vertebrate_mammalian', 'vertebrate_other', 'viral']
groups = config['groups'] if 'groups' in config else 'all'


##########################################################################
##########################################################################
##
##                                Variables
##
##########################################################################
##########################################################################

# Output folder :
if 'output_folder' in config :
    OUTPUT_FOLDER =  os.path.join(config['output_folder'], project_name)
else :
    OUTPUT_FOLDER = os.path.join(os.getcwd(), project_name)


# Seepup option that create a reduce dataset using a psiblast step with the seed 
if 'speedup' in config :
    speedup = os.path.join(OUTPUT_FOLDER, 'results', f'all_protein_{project_name}--eval_{e_val:.0e}.fasta')
else  :
    speedup = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'taxid_all_together.fasta')

##########################################################################
##########################################################################
##
##                         Script pre-process
##
##########################################################################
##########################################################################

# Definition of the list of seed
seed_table = pd.read_table(seed_file)

# Definition of the requirements for each seed
gene_constrains = ['{seed}_evalue_{eval:.0e}_cov_{coverage}_pid_{pid}'.format(seed=row.seed,
                                                                         eval=row.evalue, 
                                                                         coverage=row.coverage,
                                                                         pid=row.pident) 
                                                    for index, row in seed_table.iterrows()]

##########################################################################
##########################################################################
##
##                                Rules
##
##########################################################################
##########################################################################

rule all:
    """
    Starts the pipeline by checking the presence abscence table and the plots in pdf and png exist.
    """

    input:
        plots = expand(os.path.join(OUTPUT_FOLDER,'results','plots','gene_table_{project_name}.{ext}'), 
                                                                                 project_name=project_name, 
                                                                                 ext=['png','pdf']),
    shell:
        '''
        '''

##########################################################################
##########################################################################

rule fetch_proteins_database:
    """
    Create the protein database with all the taxid given by the user. 
    Also providing a tab-delimited file with genome metadata.

    Input : str
    -----
        list of taxid in column, no header, from the rule plit_taxid_file
        
    Output : str
    ------
        table of the information collected on the proteins, without header.
        format: protein id | protein name | genome name | genome status | genome id | taxid | length | sequence
    """

    input:
        taxid
    output:
        fasta_final = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'taxid_all_together.fasta'),
        assembly_output = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'summary_assembly_taxid.txt'),
        new_taxid_file = os.path.join(OUTPUT_FOLDER, 'taxid_checked.txt'),
        output_table_protein = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'protein_table.tsv')
    params:
        output_database_folder = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid'),
        section = section,
        assembly_levels = assembly_levels,
        refseq_categories = refseq_categories,
    resources: 
        cpus=5,
    threads :
        5
    run:
        import pandas as pd
        from ete3 import NCBITaxa 
        import ncbi_genome_download as ngd 
        import glob, os
        import gzip
        from Bio import SeqIO

        # If never done, will download the last version of NCBI Taxonomy dump and create the SQL database
        ncbi = NCBITaxa()  

        # To update the database 
        # ncbi.update_taxonomy_database()  

        # If wanted the taxdump could be remove after the database is created
        # if os.path.isfile('taxdump.tar.gz') :
        #    os.remove('taxdump.tar.gz')

        taxid_df = pd.read_table(str(input))

        new_taxid_df = pd.DataFrame()

        ### Amelioration purposes : Seems we could parse line per line the taxid file without loading it
        # For each item in the taxid file, check the lastest descendant taxa from the taxid
        # If a taxid doesn't exist, it is not added in the dataframe
        for index, row in taxid_df.iterrows():
            try :
                list_taxid = ncbi.get_descendant_taxa(row.TaxId)

                tmp_df = pd.DataFrame({'TaxId':list_taxid,
                                       'NCBIGroups': row.NCBIGroups})

                new_taxid_df = pd.concat([new_taxid_df, tmp_df])
            except ValueError :
                pass
        

        # Drop duplicate in case user have multiple time the same taxonomic id
        new_taxid_df.drop_duplicates('TaxId', inplace=True)        
        new_taxid_df.to_csv(str(output.new_taxid_file), index=False, sep='\t')

        # Dictionnary of all the option of ncbi_genome_download
        keyargs = { "section": params.section, 
                    "file_formats": "protein-fasta", 
                    'flat_output':True,  
                    "output": str(params.output_database_folder), 
                    "parallel": threads, 
                    "metadata_table":str(output.assembly_output),
                    "assembly_levels":params.assembly_levels,
                    "refseq_categories":params.refseq_categories}       
        
        # Create a dataframe that will contains all the assembly info
        assembly_final_df = pd.DataFrame()

        # If think it is better to search for all the same NCBI groups in the same time as the function can be parallized
        for NCBIgroup, group in new_taxid_df.groupby('NCBIGroups'):
            keyargs['groups'] = NCBIgroup
            keyargs['taxids'] = [str(taxid) for taxid in group.TaxId.tolist()]

            ngd.download(**keyargs)  

            # Read the information about the assembly and concatenate with previous one
            tmp_assembly = pd.read_table(str(output.assembly_output))
            assembly_final_df = pd.concat([assembly_final_df, tmp_assembly])

        assembly_final_df.to_csv(str(output.assembly_output), index=False, sep='\t')

        # Now handeling the last step that is concatenate the fasta files downloaded
        df_proteins = pd.DataFrame(columns = ['protein_id',
                                              'protein_name',
                                              'genome_name',
                                              'genome_id',
                                              'length'])

        with open(str(output.fasta_final), 'w') as w_file :
            for index, genome in assembly_final_df.iterrows() :
                with gzip.open(genome.local_filename, "rt") as handle:
                    parser = SeqIO.parse(handle, "fasta")

                    for protein in parser : 
                        description_split = protein.description.split(' [')

                        # To avoid duplicate name of proteins between close genomes
                        protein.id = f'{protein.id}--{genome.assembly_accession}'

                        df_proteins.at[-1, 'protein_id']   = protein.id
                        df_proteins.at[-1, 'protein_name'] = ' '.join(description_split[0].split(' ')[1:])
                        df_proteins.at[-1, 'genome_name']  = description_split[1].split(']')[0]
                        df_proteins.at[-1, 'genome_id']    = genome.assembly_accession
                        df_proteins.at[-1, 'length']       = len(protein.seq)

                        df_proteins.index += 1

                        SeqIO.write(protein, w_file, 'fasta')



                # Here to save space I decided to remove the file avec concatenation
                os.remove(genome.local_filename)

        df_proteins.to_csv(str(output.output_table_protein), sep='\t', index=False)



##########################################################################
##########################################################################

rule fetch_fasta_from_seed:
    """
    from the seed table and fetch the fasta of the seed. Then they are writen in the output file.

    Input : str
    -----
        the seed file input.
        table without header in the format : 
            name | protein id | e-value | percentage of identity | coverage | color
        
    Outputs : str
    -------   
        multifasta output of the seed sequences
    """
    input :
        seed_file
    output:
        os.path.join(OUTPUT_FOLDER, 'seeds_{project_name}.fasta')
    run:
        from Bio import Entrez
        import pandas as pd

        Entrez.tool = 'draw presence/abscence v2'
        Entrez.email = 'decrecylab@gmail.com'

        ### Amelioration purposes : Seems we could parse line per line the seed file without loading it
        seed_table = pd.read_table(str(input))
        id_list = seed_table.protein_id.to_list()

        # getting seed sequences and writing the fasta file
        with Entrez.efetch(db='protein', id=id_list, rettype='fasta', retmode='text') as handle:
            with open(str(output), 'w') as out_file:
                out_file.write(handle.read())

##########################################################################
##########################################################################

rule psiblast:
    """
    Use the sequences of the seeds to make a psiBLAST against all the taxid
    
    Inputs
    ------
    seed : str
        the seed multifasta file input from rule fetch_fasta_from_seed
    taxid : str
        list of taxid in columns, no header
        
    Output
    ------
    blast out : str
        blast out format in tabulation, no header
        format : query accession | query length | query sequence | query start position | querry end position |
                subject accession | subject length | subject sequence| subject start position | subject end position | 
                length of alignment | percentage of identity | e-value | bitscore | querry coverage
                
    Params
    ------
    e_val : int
        e-value threshold for psi-blast chosen by user
    blast_version : str
        blast version to use
    """

    input:
        seed = os.path.join(OUTPUT_FOLDER, 'seeds_{project_name}.fasta'),
        taxid_db = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'taxid_all_together.fasta')
    output:
        os.path.join(OUTPUT_FOLDER, 'processing_files','psiblast_{project_name}--eval_{e_val}_raw.out')
    params:
        e_val = e_val,
        blast_version = blast_version
    resources: 
        cpus=5, mem_mb='10gb', time_min='5-0'    
    threads:
        5
    shell:
        '''
        makeblastdb -dbtype prot -in '{input.taxid_db}' -parse_seqids

        psiblast -query '{input.seed}' -db '{input.taxid_db}' -evalue {e_val} \
                 -outfmt '7 qacc qlen qseq qstart qend sacc slen sseq sstart send length pident evalue bitscore qcovs' \
                 -num_threads {threads} -num_iterations 3 -out '{output}'
        '''

##########################################################################
##########################################################################

rule read_psiblast:
    """
    Read the psiBLAST, remove unwanted lines ane extract the list of matches
    
    Inputs
    ------
    psiblast : str
        blast out format in tabulation, no header, from the psiblast rule
        format : query accession | query length | query sequence | query start position | querry end position |
                subject accession | subject length | subject sequence| subject start position | subject end position | 
                length of alignment | percentage of identity | e-value | bitscore | querry coverage
            
    Outputs
    -------
    clean_blast : str
        cleaned blast out format in tabulation, no header
        format : query accession | query length | query sequence | query start position | querry end position |
                subject accession | subject length | subject sequence| subject start position | subject end position | 
                length of alignment | percentage of identity | e-value | bitscore | querry coverage
    list_all_prot : str
        list of all potein identifications gathered in the psiBLAST in column
    """

    input:
        psiblast = os.path.join(OUTPUT_FOLDER, 'processing_files','psiblast_{project_name}--eval_{e_val}_raw.out')
    output:
        clean_blast = os.path.join(OUTPUT_FOLDER, 'processing_files','psiblast_{project_name}--eval_{e_val}_cleaned.out'),
        list_all_prot = os.path.join(OUTPUT_FOLDER, 'processing_files','list_all_protein_{project_name}--eval_{e_val}.csv')
    run:
        import pandas as pd

        # Opening blastout
        blast_names = ['qacc', 'qlen', 'qseq','qstart', 'qend', 'sacc', 'slen', 'sseq', 'sstart', 'send','length',
                       'pident', 'evalue', 'bitscore', 'qcovs','qcovhsp', 'ssciname', 'sblastname', 'stitle']

        psiblast_result = pd.read_table(str(input.psiblast),
                                      comment='#',
                                      names=blast_names
                                      )

        # Cleaning blastout
        psiblast_result = psiblast_result[psiblast_result.qacc != 'Search has CONVERGED!']
        psiblast_result.to_csv(str(output.clean_blast), sep='\t', index=False)

        # Getting the list of protein matches
        all_sacc = psiblast_result.sacc.unique()

        with open(str(output.list_all_prot), 'w') as w_file :
            w_file.write('protein_id\n')

            for sacc in all_sacc :
                w_file.write(f'{sacc}\n')

##########################################################################
##########################################################################

rule make_fasta:
    """
    Create a fasta file from the psiblast results and the result of the protein information in the rule cat_proteins_info
    
    Inputs
    ------
    protein_table : str
        final table of protein information from the rule cat_proteins_info, without header.
        format: protein id | protein name | genome name | genome status | genome id | taxid | length | sequence
    list_all_prot : str
        list of all protein identifications gathered in the psiBLAST in column
        
    Outputs 
    -------
    fasta : str
        multifasta file of all the unique protein ids.
    reduced_protein_table : str
        final table of protein information with removed duplicates, without header.
        format: protein id | protein name | genome name | genome status | genome id | taxid | length | sequence
    """
    input:
        protein_fasta = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'taxid_all_together.fasta'),
        list_all_prot = os.path.join(OUTPUT_FOLDER, 'processing_files','list_all_protein_{project_name}--eval_{e_val}.csv')
    output:
        fasta = os.path.join(OUTPUT_FOLDER, 'results', 'all_protein_{project_name}--eval_{e_val}.fasta'),
    run:
        import pandas as pd
        from Bio import SeqIO

        # Get all the proteins of the database inside a dict-like structure
        all_index_fasta = SeqIO.index(str(input.protein_fasta), 'fasta')

        ### Amelioration purposes : Seems we could parse line per line the list_all_prot file without loading it
        # Opening the list of protein of interests
        proteins_of_interest = pd.read_table(str(input.list_all_prot))
        proteins_of_interest = proteins_of_interest.protein_id.tolist()

        # Filtering protein table and saving
        with open(str(output.fasta), 'w') as w_file :
            for protein in proteins_of_interest :
                SeqIO.write(all_index_fasta[protein], w_file, 'fasta')


##########################################################################
##########################################################################

rule blast:
    """
    blast all versus all of the fasta of all protein generated in the rule make_fasta
    
    Inputs
    ------
    prot_sequence : str
        multifasta file of all the unique protein ids from the rule make_fasta
    seed_fasta : str
        multifasta file of all the seeds from the rule fetch_fasta_from_seed
        
    Outputs
    -------
    blast_out : str
        output format of blast
        format: query id | subject id | percentage of identity | length of match  | mismatch | gapopen |
                query start position | query end position | subject start position | subject end position |
                e-value | bitscore
    fasta_for_blast : str
        concatenation of the 2 input multifasta files
        
    Params
    ------
    blast_version : str
        version of blast
    """

    input:
         taxid_fasta = speedup,
         seed_fasta = os.path.join(OUTPUT_FOLDER, 'seeds_{project_name}.fasta')
    output:
        blast_out = os.path.join(OUTPUT_FOLDER, 'processing_files', 'blastp_{project_name}--blast_evalue_1e-2.out'),
        fasta_for_blast = os.path.join(OUTPUT_FOLDER, 'database', 'reduce_taxid', 'all_protein_with_seeds_{project_name}.fasta')
    params:
        blast_version = blast_version
    resources: 
        cpus=5, mem_mb='10gb', time_min='5-0' 
    threads:
        5
    shell:
         """
         cat '{input.taxid_fasta}' '{input.seed_fasta}' > '{output.fasta_for_blast}'

         makeblastdb -dbtype prot -in '{output.fasta_for_blast}'

         blastp -query '{output.fasta_for_blast}' -db '{output.fasta_for_blast}' -evalue 0.01 \
                -outfmt 6 -out '{output.blast_out}' -num_threads {threads} -num_alignments 25000
         """

##########################################################################
##########################################################################

rule prepare_for_silix:
    """
    Filter the blast results from the rule blast with the threshold specified for each seed in the seed file.
    Filters include the identity score, coverage and e-value.
    Create one new filtered blast result for each seed.
    
    Inputs
    ------
    fasta : str
        multifasta of proteins with seed from the rule blast
    blast_out : str
        blast output from the rule blast
        format: query id | subject id | percentage of identity | length of match  | mismatch | gapopen |
                query start position | query end position | subject start position | subject end position |
                e-value | bitscore
        
    Output : list of str
    ------
        list of blast output filtered for each seed.
        format: query id | subject id | percentage of identity | length of match  | mismatch | gapopen |
                query start position | query end position | subject start position | subject end position |
                e-value | bitscore
    
    Params
    ------
    new_dir : str
        work directory
    project_name : str
        project name determined by the user
    """

    input:
        fasta = os.path.join(OUTPUT_FOLDER, 'database', 'reduce_taxid', 'all_protein_with_seeds_{project_name}.fasta'),
        blast_out = os.path.join(OUTPUT_FOLDER, 'processing_files', 'blastp_{project_name}--blast_evalue_1e-2.out'),
    output:
        os.path.join(OUTPUT_FOLDER, 'processing_files', 'blast_out_per_gene', 'filtered_blast_{project_name}--{seed}_evalue_{eval}_cov_{coverage}_pid_{pid}.out'),
    run:
        from Bio import SeqIO
        import pandas as pd
        import numpy as np

        # Preparing seeds
        seed_list = seed_table.protein_id.to_list()
        seed_table.set_index('protein_id', inplace=True)

        # Indexing fasta
        fasta_db = SeqIO.index(str(input.fasta), 'fasta')

        # Opening blast_out and preparation
        blast_names = ['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qstart', 'qend',
                       'sstart', 'send', 'evalue', 'bitscore']

        blast_out = pd.read_csv(str(input.blast_out), sep='\t', header=None, names=blast_names)
        
        # Don't know if necessery see later 
        # blast_out['qseqid'] = blast_out.qseqid.apply(lambda x: x.split('.')[0])
        # blast_out['sseqid'] = blast_out.sseqid.apply(lambda x: x.split('.')[0])
        
        # I think it is totally useless
        # blast_out.drop_duplicates(inplace=True)

        # start filtering blast out on e-value, coverage and percent identity
        blast_out = blast_out[blast_out.evalue <= float(wildcards.eval)].reset_index(drop = True)
        blast_out = blast_out[blast_out.pident >= float(wildcards.pid)].reset_index(drop = True)

        # Calculating the coverage on the query
        for index, row in blast_out.iterrows() :
            blast_out.at[index, 'coverage'] = row.length / len(fasta_db[row.qseqid])

        blast_out = blast_out[blast_out.coverage >= float(wildcards.coverage)].reset_index(drop = True)
        
        # Write the blast_out without the column cov because could messup in silix
        blast_out[blast_names].to_csv(str(output), sep='\t', index=False, header=False)

##########################################################################
##########################################################################

rule silix:
    """
    Uses Silix to create a network of protein and give a file of the protein segregated in groups.
    If the blast output file is empty, just create an empty file
    
    Inputs
    ------
    blast_out : str
        blast output filtered for a specific seed from the rule prepare_for_silix.
        format: query id | subject id | percentage of identity | length of match  | mismatch | gapopen |
                query start position | query end position | subject start position | subject end position |
                e-value | bitscore
    fasta : str
        multifasta of proteins with seed from the rule blast
        
    Output : str
    ------
        fnodes file, table of protein id and family number, without headers.
        format: family | protein id
        
    Params
    ------
    silix_version : str
        version of silix to use
    """

    input:
        fasta = os.path.join(OUTPUT_FOLDER, 'database', 'reduce_taxid', 'all_protein_with_seeds_{project_name}.fasta'),
        blast_out = os.path.join(OUTPUT_FOLDER, 'processing_files','blast_out_per_gene', 'filtered_blast_{project_name}--{seed}_evalue_{eval}_cov_{coverage}_pid_{pid}.out'),
    output:
        os.path.join(OUTPUT_FOLDER, 'processing_files','blast_out_per_gene', 'filtered_blast_{project_name}--{seed}_evalue_{eval}_cov_{coverage}_pid_{pid}.fnodes')
    params:
        silix_version = silix_version
    shell:
        """
        if [ -s {input.blast_out} ]
        then   
            sh -c 'silix "{input.fasta}" "{input.blast_out}" -f "{wildcards.seed}" -i 0.05 -r 0.05 > "{output}"'
        else
            touch '{output}'
        fi
        """

##########################################################################
##########################################################################

rule find_family:
    """
    Find the group of each seed in each individual seed and record it
    
    Input
    -----
    fnodes : str
        fnodes file, table of protein id and family number, without headers from the rule silix.
        format: family | protein id
        
    Output : str
    ------
        updated fnodes with only the family of the seed.
        format: family | protein id | seed
    """

    input:
        fnodes = os.path.join(OUTPUT_FOLDER, 'processing_files','blast_out_per_gene', 'filtered_blast_{project_name}--{seed}_evalue_{eval}_cov_{coverage}_pid_{pid}.fnodes'),
        seed_file = seed_file
    output:
        os.path.join(OUTPUT_FOLDER, 'processing_files','blast_out_per_gene', 'filtered_blast_{project_name}--{seed}_evalue_{eval}_cov_{coverage}_pid_{pid}.fnodes.flushed')
    run:
        # Open fnodes
        fnodes = pd.read_table(str(input.fnodes),names=['family', 'protein_id'])

        seed = wildcards.seed

        ### Amelioration purposes : Seems we could parse line per line the taxid file without loading it
        seed_df = pd.read_table(str(input.seed_file))
        seed_protname = seed_df.set_index('seed').protein_id.to_dict()[seed]

        # Detection families WARNING verification about the name of the seed and the name of the protein in the fasta at the end may be different
        gene_family = fnodes.loc[fnodes.protein_id.str.contains(seed_protname), 'family'].unique()[0]

        # writing file with only family
        fnodes = fnodes.loc[fnodes.family == gene_family]

        # Creation of a columns 'seed' to identify quickly the seed and familiy
        fnodes['seed'] = seed

        fnodes.to_csv(str(output), sep='\t', index=False)

##########################################################################
##########################################################################

rule make_table:
    """
    Check the presence of protein similar to the seed in each taxid and create a table of presence abscence
    This table is then plotted in a colored table.
    
    Inputs
    ------
    protein_table : str
        final table of protein information from the rule cat_proteins_info, without header.
        format: protein id | protein name | genome name | genome status | genome id | taxid | length | sequence
    fnode : str
        concatenated fnodes with each seed family from 
        format: family | protein id | seed
            
    Outputs
    -------
    final_table : str
        presence/abscence table, with header. Each line is a genome, each column is a seed.
        format: genome id | genome name | seed 1 | seed 2 .. seed x
    pdf : list of str
        plots in pdf of the final table centered on one seed
    png : list of str
        plots in png of the final table centered on one seed
    """

    input:
        seed_file = seed_file,
        protein_table = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'protein_table.tsv'),
        assembly_table = os.path.join(OUTPUT_FOLDER, 'database', 'all_taxid', 'summary_assembly_taxid.txt'),
        fnodes = expand(os.path.join(OUTPUT_FOLDER, 
                                    'processing_files','blast_out_per_gene', 
                                    'filtered_blast_{project_name}--{gene_constrains}.fnodes.flushed'),
                                     project_name=project_name, gene_constrains=gene_constrains)
    output:
        final_table = os.path.join(OUTPUT_FOLDER, 'results', 'patab_{project_name}.tsv'),
    run:
        import pandas as pd
        import numpy as np

        # Seed preparing
        seed_table = pd.read_table(str(input.seed_file))
        seed_list = seed_table.seed.to_list()
        seed_color_dict = seed_table.set_index('seed').color.to_dict()

        # list of all proteins
        all_proteins = pd.read_table(str(input.protein_table))

        # fnodes opening
        fnodes_files = [pd.read_table(fnodes_file) for fnodes_file in input.fnodes]
        
        # concat all fnodes dataframe to one and add the protein information from the protein table and genome table
        fam_id_table = pd.concat(fnodes_files)
        fam_id_table = fam_id_table.merge(all_proteins, on='protein_id')

        patab = pd.crosstab(index = fam_id_table['genome_id'], columns = fam_id_table['seed'])

        # To add missing seed if not find
        seed_missing = [seed for seed in seed_list if seed not in patab.columns]
        patab.loc[:,seed_missing] = 0

        patab = patab[seed_list].sort_values(by = seed_list, ascending = False).reset_index()

        # Add the genome name to the table in case needed
        patab = patab.merge(fam_id_table[['genome_id','genome_name']].drop_duplicates(), on='genome_id')
        
        patab = patab.melt(id_vars=['genome_id', 'genome_name'], var_name='seed', value_name='PA')

        # Put color instead of number
        for index, row in patab.iterrows():
            # Use the fact that 0 == False in python to test if it's 1 or 0
            if row.PA :
                patab.at[index, 'color'] = seed_color_dict[row.seed]
            else :
                patab.at[index, 'color'] = '#FFFFFF' # White color

        # save the table
        patab.to_csv(str(output.final_table), sep='\t', index=False)

##########################################################################
##########################################################################


rule plots :
    input :
        final_table = os.path.join(OUTPUT_FOLDER, 'results', f'patab_{project_name}.tsv')
    output :
        pdf = os.path.join(OUTPUT_FOLDER,'results','plots','gene_table_{project_name}.pdf'), 
        png = os.path.join(OUTPUT_FOLDER,'results','plots','gene_table_{project_name}.png'),
    run :
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt

        # It seems there is a bug if another backend is used
        import matplotlib
        matplotlib.use('Agg')

        # Plot parameters
        font = {'family': 'DejaVu Sans', 'weight': 'light', 'size': 12, }
        plt.rc('font',**font)
        plt.rcParams['text.color'] = 'black'
        plt.rcParams['svg.fonttype'] = 'none'  # Editable SVG text


        # Name in PAtab genome_id, seed, PA, color, genome_name
        # Dans le truc de geoffrey, number = nombre de gene present, x_pos = l'endroit dans la liste des seed, y_pos = l'endroit dans les genomes

        # figsize = (width, height)
        fig, ax = plt.subplots(1,1, figsize=(7.5, 8.75))

        label_format = {'fontweight': 'bold'}

        patab = pd.read_table(str(input.final_table))

        # Dict position genomes and gene
        list_genome = patab.genome_id.unique().tolist() 

        ### Amelioration purposes : The genome name could be followed by the id in parenthesis
        ### or we could ask the user for id or name in the figure
        
        # As some genome name could be the same I need to create a dict to convert the name
        genomeId_2_genomeName = patab.set_index('genome_id').genome_name.to_dict()
        list_genome_name = [genomeId_2_genomeName[genome] for genome in list_genome] 

        num_genome = len(list_genome)
        # here fist genome on top
        dict_pos_genome = {list_genome[index]:num_genome-index-1 for index in range(num_genome)}

        list_seed = patab.seed.unique().tolist()
        num_seed = len(list_seed)
        dict_pos_seed = {list_seed[index]:index for index in range(num_seed)}

        for _, row in patab.iterrows():
            ax.plot(dict_pos_seed[row.seed],
                    dict_pos_genome[row.genome_id],
                    linestyle="None",marker="s",
                    markersize=15, mfc=row.color, 
                    mec='black',markeredgewidth=1)

            if row.PA > 1:
               ax.text(x = dict_pos_seed[row.seed],
                       y = dict_pos_genome[row.genome_id],
                       s = str(row.PA),fontsize=11,
                       color='white',ha='center',va='center',
                       fontweight='heavy')

        plt.yticks(range(num_genome),list_genome_name[::-1],**label_format)
        plt.xticks(range(num_seed),list_seed,**label_format)

        ax.tick_params(axis='both',which='both',length=0)  # No tick markers
        ax.set_ylabel('')  # No ylabel
        ax.xaxis.tick_top()  # xticklabels on top
        ax.xaxis.set_label_position('top')
        plt.setp(ax.xaxis.get_majorticklabels(),rotation=90,ha='center')  # Rotate x labels

        for pos in ['top', 'bottom', 'left', 'right']:
            ax.spines[pos].set_visible(False)  # Remove border

        plt.xlim(-0.5,num_seed - 0.5)
        plt.ylim(-0.5,num_genome - 0.5)

        for plot_name in output :
            plt.savefig(plot_name, bbox_inches="tight",dpi=300)

        return 

        
